Start infering...
2025-06-07 17:36:15 | INFO | numexpr.utils | Note: detected 112 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2025-06-07 17:36:15 | INFO | numexpr.utils | Note: NumExpr detected 112 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-06-07 17:36:15 | INFO | unimat.inference | loading model(s) from ../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/cv_seed_42_fold_0/checkpoint_best.pt
2025-06-07 17:36:16 | INFO | uninmr_fnmr.tasks.uninmr | dictionary: 30 types
2025-06-07 17:36:16 | INFO | unimat.inference | Namespace(no_progress_bar=False, log_interval=50, log_format='simple', tensorboard_logdir='', wandb_project='', wandb_name='', seed=1, cpu=False, fp16=True, bf16=False, bf16_sr=False, allreduce_fp32_grad=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=256, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir='../uninmr_fnmr', empty_cache_freq=0, all_gather_list_size=16384, suppress_crashes=False, profile=False, ema_decay=-1.0, validate_with_ema=False, loss='atom_regloss_mae', optimizer='adam', lr_scheduler='fixed', task='uninmr', num_workers=8, skip_invalid_size_inputs_valid_test=False, batch_size=16, required_batch_size_multiple=1, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, batch_size_valid=16, max_valid_steps=None, curriculum=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, nprocs_per_node=2, path='../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/cv_seed_42_fold_0/checkpoint_best.pt', quiet=False, model_overrides='{}', results_path='../output/test/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/0', arch='unimol_large', data='../data/test', saved_dir='../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5', classification_head_name='nmr_head', num_classes=1, max_atoms=512, dict_name='dict.txt', remove_hydrogen=False, has_matid=False, conformer_augmentation=False, conf_size=10, global_distance=False, atom_descriptor=0, selected_atom='F', split_mode='infer', nfolds=5, fold=0, cv_seed=42, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, gaussian_kernel=True, no_seed_provided=False, encoder_layers=15, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=64, dropout=0.1, emb_dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, max_seq_len=1024, activation_fn='gelu', pooler_activation_fn='tanh', post_ln=False, masked_token_loss=-1.0, masked_coord_loss=-1.0, masked_dist_loss=-1.0, x_norm_loss=-1.0, delta_pair_repr_norm_loss=-1.0, lattice_loss=-1.0, distributed_num_procs=1)
2025-06-07 17:36:16 | INFO | unimat.inference | UniMatModel(
  (embed_tokens): Embedding(31, 512, padding_idx=0)
  (encoder): TransformerEncoderWithPair(
    (emb_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0-14): 15 x TransformerEncoderLayer(
        (self_attn): SelfMultiheadAttention(
          (in_proj): Linear(in_features=512, out_features=1536, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (gbf_proj): NonLinearHead(
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (linear2): Linear(in_features=128, out_features=64, bias=True)
  )
  (gbf): GaussianLayer(
    (means): Embedding(1, 128)
    (stds): Embedding(1, 128)
    (mul): Embedding(961, 1)
    (bias): Embedding(961, 1)
  )
  (classification_heads): ModuleDict()
  (node_classification_heads): ModuleDict(
    (nmr_head): NodeClassificationHead(
      (dense): Linear(in_features=512, out_features=512, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (out_proj): Linear(in_features=512, out_features=1, bias=True)
    )
  )
)
2025-06-07 17:36:16 | INFO | unimat.inference | task: UniNMRTask
2025-06-07 17:36:16 | INFO | unimat.inference | model: UniMatModel
2025-06-07 17:36:16 | INFO | unimat.inference | loss: AtomRegMAELoss
2025-06-07 17:36:16 | INFO | unimat.inference | num. model params: 47,593,795 (num. trained: 47,593,795)
2025-06-07 17:36:16 | INFO | unicore.tasks.unicore_task | get EpochBatchIterator for epoch 1
2025-06-07 17:36:20 | INFO | unimat.inference | Done inference! 
Start infering...
2025-06-07 17:36:26 | INFO | numexpr.utils | Note: detected 112 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2025-06-07 17:36:26 | INFO | numexpr.utils | Note: NumExpr detected 112 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-06-07 17:36:26 | INFO | unimat.inference | loading model(s) from ../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/cv_seed_42_fold_1/checkpoint_best.pt
2025-06-07 17:36:26 | INFO | uninmr_fnmr.tasks.uninmr | dictionary: 30 types
2025-06-07 17:36:28 | INFO | unimat.inference | Namespace(no_progress_bar=False, log_interval=50, log_format='simple', tensorboard_logdir='', wandb_project='', wandb_name='', seed=1, cpu=False, fp16=True, bf16=False, bf16_sr=False, allreduce_fp32_grad=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=256, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir='../uninmr_fnmr', empty_cache_freq=0, all_gather_list_size=16384, suppress_crashes=False, profile=False, ema_decay=-1.0, validate_with_ema=False, loss='atom_regloss_mae', optimizer='adam', lr_scheduler='fixed', task='uninmr', num_workers=8, skip_invalid_size_inputs_valid_test=False, batch_size=16, required_batch_size_multiple=1, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, batch_size_valid=16, max_valid_steps=None, curriculum=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, nprocs_per_node=2, path='../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/cv_seed_42_fold_1/checkpoint_best.pt', quiet=False, model_overrides='{}', results_path='../output/test/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/1', arch='unimol_large', data='../data/test', saved_dir='../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5', classification_head_name='nmr_head', num_classes=1, max_atoms=512, dict_name='dict.txt', remove_hydrogen=False, has_matid=False, conformer_augmentation=False, conf_size=10, global_distance=False, atom_descriptor=0, selected_atom='F', split_mode='infer', nfolds=5, fold=0, cv_seed=42, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, gaussian_kernel=True, no_seed_provided=False, encoder_layers=15, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=64, dropout=0.1, emb_dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, max_seq_len=1024, activation_fn='gelu', pooler_activation_fn='tanh', post_ln=False, masked_token_loss=-1.0, masked_coord_loss=-1.0, masked_dist_loss=-1.0, x_norm_loss=-1.0, delta_pair_repr_norm_loss=-1.0, lattice_loss=-1.0, distributed_num_procs=1)
2025-06-07 17:36:28 | INFO | unimat.inference | UniMatModel(
  (embed_tokens): Embedding(31, 512, padding_idx=0)
  (encoder): TransformerEncoderWithPair(
    (emb_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0-14): 15 x TransformerEncoderLayer(
        (self_attn): SelfMultiheadAttention(
          (in_proj): Linear(in_features=512, out_features=1536, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (gbf_proj): NonLinearHead(
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (linear2): Linear(in_features=128, out_features=64, bias=True)
  )
  (gbf): GaussianLayer(
    (means): Embedding(1, 128)
    (stds): Embedding(1, 128)
    (mul): Embedding(961, 1)
    (bias): Embedding(961, 1)
  )
  (classification_heads): ModuleDict()
  (node_classification_heads): ModuleDict(
    (nmr_head): NodeClassificationHead(
      (dense): Linear(in_features=512, out_features=512, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (out_proj): Linear(in_features=512, out_features=1, bias=True)
    )
  )
)
2025-06-07 17:36:28 | INFO | unimat.inference | task: UniNMRTask
2025-06-07 17:36:28 | INFO | unimat.inference | model: UniMatModel
2025-06-07 17:36:28 | INFO | unimat.inference | loss: AtomRegMAELoss
2025-06-07 17:36:28 | INFO | unimat.inference | num. model params: 47,593,795 (num. trained: 47,593,795)
2025-06-07 17:36:28 | INFO | unicore.tasks.unicore_task | get EpochBatchIterator for epoch 1
2025-06-07 17:36:30 | INFO | unimat.inference | Done inference! 
Start infering...
2025-06-07 17:36:36 | INFO | numexpr.utils | Note: detected 112 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2025-06-07 17:36:36 | INFO | numexpr.utils | Note: NumExpr detected 112 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-06-07 17:36:37 | INFO | unimat.inference | loading model(s) from ../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/cv_seed_42_fold_2/checkpoint_best.pt
2025-06-07 17:36:37 | INFO | uninmr_fnmr.tasks.uninmr | dictionary: 30 types
2025-06-07 17:36:38 | INFO | unimat.inference | Namespace(no_progress_bar=False, log_interval=50, log_format='simple', tensorboard_logdir='', wandb_project='', wandb_name='', seed=1, cpu=False, fp16=True, bf16=False, bf16_sr=False, allreduce_fp32_grad=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=256, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir='../uninmr_fnmr', empty_cache_freq=0, all_gather_list_size=16384, suppress_crashes=False, profile=False, ema_decay=-1.0, validate_with_ema=False, loss='atom_regloss_mae', optimizer='adam', lr_scheduler='fixed', task='uninmr', num_workers=8, skip_invalid_size_inputs_valid_test=False, batch_size=16, required_batch_size_multiple=1, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, batch_size_valid=16, max_valid_steps=None, curriculum=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, nprocs_per_node=2, path='../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/cv_seed_42_fold_2/checkpoint_best.pt', quiet=False, model_overrides='{}', results_path='../output/test/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/2', arch='unimol_large', data='../data/test', saved_dir='../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5', classification_head_name='nmr_head', num_classes=1, max_atoms=512, dict_name='dict.txt', remove_hydrogen=False, has_matid=False, conformer_augmentation=False, conf_size=10, global_distance=False, atom_descriptor=0, selected_atom='F', split_mode='infer', nfolds=5, fold=0, cv_seed=42, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, gaussian_kernel=True, no_seed_provided=False, encoder_layers=15, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=64, dropout=0.1, emb_dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, max_seq_len=1024, activation_fn='gelu', pooler_activation_fn='tanh', post_ln=False, masked_token_loss=-1.0, masked_coord_loss=-1.0, masked_dist_loss=-1.0, x_norm_loss=-1.0, delta_pair_repr_norm_loss=-1.0, lattice_loss=-1.0, distributed_num_procs=1)
2025-06-07 17:36:38 | INFO | unimat.inference | UniMatModel(
  (embed_tokens): Embedding(31, 512, padding_idx=0)
  (encoder): TransformerEncoderWithPair(
    (emb_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0-14): 15 x TransformerEncoderLayer(
        (self_attn): SelfMultiheadAttention(
          (in_proj): Linear(in_features=512, out_features=1536, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (gbf_proj): NonLinearHead(
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (linear2): Linear(in_features=128, out_features=64, bias=True)
  )
  (gbf): GaussianLayer(
    (means): Embedding(1, 128)
    (stds): Embedding(1, 128)
    (mul): Embedding(961, 1)
    (bias): Embedding(961, 1)
  )
  (classification_heads): ModuleDict()
  (node_classification_heads): ModuleDict(
    (nmr_head): NodeClassificationHead(
      (dense): Linear(in_features=512, out_features=512, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (out_proj): Linear(in_features=512, out_features=1, bias=True)
    )
  )
)
2025-06-07 17:36:38 | INFO | unimat.inference | task: UniNMRTask
2025-06-07 17:36:38 | INFO | unimat.inference | model: UniMatModel
2025-06-07 17:36:38 | INFO | unimat.inference | loss: AtomRegMAELoss
2025-06-07 17:36:38 | INFO | unimat.inference | num. model params: 47,593,795 (num. trained: 47,593,795)
2025-06-07 17:36:38 | INFO | unicore.tasks.unicore_task | get EpochBatchIterator for epoch 1
2025-06-07 17:36:41 | INFO | unimat.inference | Done inference! 
Start infering...
2025-06-07 17:36:46 | INFO | numexpr.utils | Note: detected 112 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2025-06-07 17:36:46 | INFO | numexpr.utils | Note: NumExpr detected 112 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-06-07 17:36:46 | INFO | unimat.inference | loading model(s) from ../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/cv_seed_42_fold_3/checkpoint_best.pt
2025-06-07 17:36:47 | INFO | uninmr_fnmr.tasks.uninmr | dictionary: 30 types
2025-06-07 17:36:48 | INFO | unimat.inference | Namespace(no_progress_bar=False, log_interval=50, log_format='simple', tensorboard_logdir='', wandb_project='', wandb_name='', seed=1, cpu=False, fp16=True, bf16=False, bf16_sr=False, allreduce_fp32_grad=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=256, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir='../uninmr_fnmr', empty_cache_freq=0, all_gather_list_size=16384, suppress_crashes=False, profile=False, ema_decay=-1.0, validate_with_ema=False, loss='atom_regloss_mae', optimizer='adam', lr_scheduler='fixed', task='uninmr', num_workers=8, skip_invalid_size_inputs_valid_test=False, batch_size=16, required_batch_size_multiple=1, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, batch_size_valid=16, max_valid_steps=None, curriculum=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, nprocs_per_node=2, path='../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/cv_seed_42_fold_3/checkpoint_best.pt', quiet=False, model_overrides='{}', results_path='../output/test/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/3', arch='unimol_large', data='../data/test', saved_dir='../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5', classification_head_name='nmr_head', num_classes=1, max_atoms=512, dict_name='dict.txt', remove_hydrogen=False, has_matid=False, conformer_augmentation=False, conf_size=10, global_distance=False, atom_descriptor=0, selected_atom='F', split_mode='infer', nfolds=5, fold=0, cv_seed=42, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, gaussian_kernel=True, no_seed_provided=False, encoder_layers=15, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=64, dropout=0.1, emb_dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, max_seq_len=1024, activation_fn='gelu', pooler_activation_fn='tanh', post_ln=False, masked_token_loss=-1.0, masked_coord_loss=-1.0, masked_dist_loss=-1.0, x_norm_loss=-1.0, delta_pair_repr_norm_loss=-1.0, lattice_loss=-1.0, distributed_num_procs=1)
2025-06-07 17:36:48 | INFO | unimat.inference | UniMatModel(
  (embed_tokens): Embedding(31, 512, padding_idx=0)
  (encoder): TransformerEncoderWithPair(
    (emb_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0-14): 15 x TransformerEncoderLayer(
        (self_attn): SelfMultiheadAttention(
          (in_proj): Linear(in_features=512, out_features=1536, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (gbf_proj): NonLinearHead(
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (linear2): Linear(in_features=128, out_features=64, bias=True)
  )
  (gbf): GaussianLayer(
    (means): Embedding(1, 128)
    (stds): Embedding(1, 128)
    (mul): Embedding(961, 1)
    (bias): Embedding(961, 1)
  )
  (classification_heads): ModuleDict()
  (node_classification_heads): ModuleDict(
    (nmr_head): NodeClassificationHead(
      (dense): Linear(in_features=512, out_features=512, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (out_proj): Linear(in_features=512, out_features=1, bias=True)
    )
  )
)
2025-06-07 17:36:48 | INFO | unimat.inference | task: UniNMRTask
2025-06-07 17:36:48 | INFO | unimat.inference | model: UniMatModel
2025-06-07 17:36:48 | INFO | unimat.inference | loss: AtomRegMAELoss
2025-06-07 17:36:48 | INFO | unimat.inference | num. model params: 47,593,795 (num. trained: 47,593,795)
2025-06-07 17:36:48 | INFO | unicore.tasks.unicore_task | get EpochBatchIterator for epoch 1
2025-06-07 17:36:51 | INFO | unimat.inference | Done inference! 
Start infering...
2025-06-07 17:36:57 | INFO | numexpr.utils | Note: detected 112 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2025-06-07 17:36:57 | INFO | numexpr.utils | Note: NumExpr detected 112 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-06-07 17:36:57 | INFO | unimat.inference | loading model(s) from ../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/cv_seed_42_fold_4/checkpoint_best.pt
2025-06-07 17:36:58 | INFO | uninmr_fnmr.tasks.uninmr | dictionary: 30 types
2025-06-07 17:36:59 | INFO | unimat.inference | Namespace(no_progress_bar=False, log_interval=50, log_format='simple', tensorboard_logdir='', wandb_project='', wandb_name='', seed=1, cpu=False, fp16=True, bf16=False, bf16_sr=False, allreduce_fp32_grad=False, fp16_no_flatten_grads=False, fp16_init_scale=4, fp16_scale_window=256, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir='../uninmr_fnmr', empty_cache_freq=0, all_gather_list_size=16384, suppress_crashes=False, profile=False, ema_decay=-1.0, validate_with_ema=False, loss='atom_regloss_mae', optimizer='adam', lr_scheduler='fixed', task='uninmr', num_workers=8, skip_invalid_size_inputs_valid_test=False, batch_size=16, required_batch_size_multiple=1, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, batch_size_valid=16, max_valid_steps=None, curriculum=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='no_c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, fast_stat_sync=False, broadcast_buffers=False, nprocs_per_node=2, path='../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/cv_seed_42_fold_4/checkpoint_best.pt', quiet=False, model_overrides='{}', results_path='../output/test/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5/4', arch='unimol_large', data='../data/test', saved_dir='../output/finetune/5cv/F_pretraining_molecular_global__kener__atomdes__unimol_large_atom_regloss_mse_lr_0.0001_bs_16_0.06_5', classification_head_name='nmr_head', num_classes=1, max_atoms=512, dict_name='dict.txt', remove_hydrogen=False, has_matid=False, conformer_augmentation=False, conf_size=10, global_distance=False, atom_descriptor=0, selected_atom='F', split_mode='infer', nfolds=5, fold=0, cv_seed=42, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, gaussian_kernel=True, no_seed_provided=False, encoder_layers=15, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_attention_heads=64, dropout=0.1, emb_dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, max_seq_len=1024, activation_fn='gelu', pooler_activation_fn='tanh', post_ln=False, masked_token_loss=-1.0, masked_coord_loss=-1.0, masked_dist_loss=-1.0, x_norm_loss=-1.0, delta_pair_repr_norm_loss=-1.0, lattice_loss=-1.0, distributed_num_procs=1)
2025-06-07 17:36:59 | INFO | unimat.inference | UniMatModel(
  (embed_tokens): Embedding(31, 512, padding_idx=0)
  (encoder): TransformerEncoderWithPair(
    (emb_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0-14): 15 x TransformerEncoderLayer(
        (self_attn): SelfMultiheadAttention(
          (in_proj): Linear(in_features=512, out_features=1536, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (gbf_proj): NonLinearHead(
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (linear2): Linear(in_features=128, out_features=64, bias=True)
  )
  (gbf): GaussianLayer(
    (means): Embedding(1, 128)
    (stds): Embedding(1, 128)
    (mul): Embedding(961, 1)
    (bias): Embedding(961, 1)
  )
  (classification_heads): ModuleDict()
  (node_classification_heads): ModuleDict(
    (nmr_head): NodeClassificationHead(
      (dense): Linear(in_features=512, out_features=512, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (out_proj): Linear(in_features=512, out_features=1, bias=True)
    )
  )
)
2025-06-07 17:36:59 | INFO | unimat.inference | task: UniNMRTask
2025-06-07 17:36:59 | INFO | unimat.inference | model: UniMatModel
2025-06-07 17:36:59 | INFO | unimat.inference | loss: AtomRegMAELoss
2025-06-07 17:36:59 | INFO | unimat.inference | num. model params: 47,593,795 (num. trained: 47,593,795)
2025-06-07 17:36:59 | INFO | unicore.tasks.unicore_task | get EpochBatchIterator for epoch 1
2025-06-07 17:37:01 | INFO | unimat.inference | Done inference! 
